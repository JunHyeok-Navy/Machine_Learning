{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Denoising Diffusion Probabilistic Model with Tensorflow",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMziPAl5DOzgWu8uF5LvBbW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunHyeok-Navy/Machine_Learning/blob/main/Denoising_Diffusion_Probabilistic_Model_with_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWJvigCRE55K",
        "outputId": "30187b49-998a-4a67-bea8-a45858f56b8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 30.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "Ejrqf6m0Me_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3efd50-8899-4ea6-9d72-ea0edf080374"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denoising Diffusion Probabilistic Model with Tensorflow"
      ],
      "metadata": {
        "id": "xs2fc_im-nd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract\n",
        "\n",
        "<center><img src=\"https://hojonathanho.github.io/diffusion/assets/img/celebahq_combined_6.png\" title=\"Generated by DDPM model\" width=\"400\"/></center> \n",
        "\n",
        "</br>\n",
        "\n",
        "Thesedays, State-Of-The-Art Text-to-Image models show us high-quality generated images!  \n",
        "\n",
        "When it comes to image generating model, we are familiar with GAN or VAE.  \n",
        "\n",
        "\n",
        "Since 2020, research on the diffusion model has been actively conducted, and gradually, it shows performance that is comparable to GAN.\n",
        "\n",
        "Espacially, google released SOTA Text-to-Image model ***'Imagen'*** and it is consisted of diffusion model architecture!\n",
        "\n",
        "\n",
        "From now on, we will look at the basic contents of the **diffusion model** for advanced concepts of SOTA models and implement it through **Tensorflow**.\n",
        "\n",
        "---\n",
        "\n",
        "Reference Paper\n",
        "\n",
        "- [Denoising Diffusion Probabilitics Model](https://arxiv.org/abs/2006.11239)\n",
        "- [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585)\n"
      ],
      "metadata": {
        "id": "7kVzvexO-7ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contents\n",
        "\n",
        "\n",
        "\n",
        "1.  Introduction\n",
        "2.  Mathematics & Workflow\n",
        "3.  Implementation\n",
        "4.  TPU Setting\n",
        "5.  Data Processing\n",
        "6.  Train Diffusion Model\n",
        "7.  Evaluate Sample"
      ],
      "metadata": {
        "id": "Dcp4i9jdDlNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "PKnWDpy7EPsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://i.imgur.com/WbwRmKH.jpg\" title=\"Graphical Image\" width=\"500\"></center>"
      ],
      "metadata": {
        "id": "_Jjk1hATE10W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the photo above, there is a **forward & backward** process in DDPM(Denoising Diffusion Probabilistic Model).\n",
        "\n",
        "In forward process, model gradually add a little gaussian noise to original input image($x_0$ above) for numerous times.  \n",
        "After adding noise for T(given steps) times, your original image will be filled with full of gaussian noise!\n",
        "\n",
        "Then, DDPM find the way that restore changed noise-image to original image.  \n",
        "***In a word, model is trained how \"Backward Process\" is work!***\n",
        "\n",
        "### In the next chapter, we will learn more about the forward and backward processes and define the loss function and workflow to train the model."
      ],
      "metadata": {
        "id": "TMqRvMp1FLn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Mathematics & Workflow\n",
        "\n",
        "#### Prerequisite\n",
        "- [KL Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
        "- [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)\n",
        "- [Markov Chain](https://brilliant.org/wiki/markov-chains/)\n",
        "\n",
        "You need a little bit mathematical knowledge in statistic & calculus.  \n",
        "I'll not explain all about detail math lines in this notebook.  \n",
        "You don't have to study whole thing. If there is a incomprehensible concept, you can search it at google or youtube for learning! \n",
        "There are so many good tutorials or resources. So don't worry about it :)"
      ],
      "metadata": {
        "id": "H0BSRX7fNI7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Process\n",
        "\n",
        "$:=$ means 'define'.  \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "q(x_t|x_{t-1}) &:= N(x_t ; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_tI)\\\\\n",
        "q(x_{1:T}|x_0) &:= \\Pi_{t=1}^{T}{q(x_t|x_{t-1})}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "</br>\n",
        "\n",
        "$\\beta_t$ is the variance schedule which will be fixed to a constant.\n",
        "\n",
        "Using [Reparameterization Trick](https://gregorygundersen.com/blog/2018/04/29/reparameterization/), we can sample $x_t$ using above definition.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "q(x_t|x_{t-1}) &:= N(x_t ; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_tI)\\\\\n",
        "x_t &= \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}z_{t-1} (Where, \\alpha_t =1-\\beta_t, \\bar{α_t} = \\Pi_{s=1}^{t}{}\\alpha_s)\\\\\n",
        "x_{t-1} &= \\sqrt{\\alpha_t \\alpha_{t-1}}x_{t-2} + \\sqrt{1-\\alpha_t \\alpha_{t-1}}z_{t-2}\\\\\n",
        "\\therefore x_t &= \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}z\n",
        "\\end{aligned}$$\n",
        "\n",
        "</br>\n",
        "\n",
        "So, we can handle the sample at timestep $t$ with below.\n",
        "\n",
        "<center>$q(x_t|x_0) = N(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha_t}I))$</center>"
      ],
      "metadata": {
        "id": "stq8yB4YSGIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reverse Process\n",
        "\n",
        "At the end of forward process, original image will be fully changed to a gaussian noise.  \n",
        "\n",
        "$$p(x_T) = N(x_t; 0, I)$$  \n",
        "\n",
        "We will predict the information (mean, variance) about the distribution of each forward process with a model, and we will represent the mean and variance of the reverse process with unknown functions, mu and sigma.  \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p_{\\theta}(x_{t-1}|x_t) &:= N(x_{t-1}; \\mu(x_t, t), \\Sigma_\\theta (x_t, t))\\\\\n",
        "p_\\theta(x_{0:T}) &= p_\\theta(x_T) \\Pi_{t=1}^{T}{p_\\theta(x_{t-1}|x_t)}\\\\\n",
        "p_\\theta(x_0) &:= \\int p_\\theta(x_{0:T})dx_{1:T}\\\\\n",
        "\\end{aligned}$$"
      ],
      "metadata": {
        "id": "PmCfvKoHbstf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss\n",
        "\n",
        "We can derive the final loss function through the following list.\n",
        "\n",
        "- [Negative log likelihood](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)\n",
        "- Evidence Lower Bound(ELBO)\n",
        "- KL Divergence\n",
        "\n",
        "### **There are so many math lines.. Hold tight!**\n",
        "If you feel confused or cannot understand easily, contact me!\n",
        "gylee206@gmail.com  \n",
        "\n",
        "</br>\n",
        "\n",
        "### <center>Let's begin!</center>   \n",
        "\n",
        "## ***1) Define loss using negative log likelihood & ELBO***\n",
        "\n",
        "</br>\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "log p_\\theta(x_0) &= log{{p_\\theta(x_0|x_{1:T})p_\\theta(x_{1:T})}\\over{p_\\theta(x_{1:T}|x_0)}}\\\\\n",
        "&= logp_\\theta(x_0|x_{1:T}) + logp_\\theta(x_{1:T})-logp_\\theta(x_{1:T}|x_0)\\\\\n",
        "E_q[logp_\\theta(x_0)] &= \\int{q(x_{1:T}|x_0)logp_\\theta(x_0)}dx_{1:T}\\\\\n",
        "&= \\int{q(x_{1:T}|x_0)}[logp_\\theta(x_0|x_{1:T}) + logp_\\theta(x_{1:T}) - logp_\\theta(x_{1:T}|x_0)]dx_{1:T}\\\\\n",
        "&= E_q[logp_\\theta(x_0|x_{1:T})] + \\int{q(x_{1:T}|x_0)logp_\\theta(x_{1:T})}dx_{1:T} - \\int{q(x_{1:T}|x_0)logp_\\theta(x_{1:T}|x_0)}dx_{1:T}\\\\\n",
        "&= E_q[logp_\\theta(x_0|x_{1:T})] + \\int{q(x_{1:T}|x_0)logp_\\theta(x_{1:T})}dx_{1:T} - \\int{q(x_{1:T}|x_0)logp_\\theta(x_{1:T}|x_0)}dx_{1:T} \\\\ &+ \\int{q(x_{1:T}|x_0)logq(x_{1:T}|x_0)}dx_{1:T} - \\int{q(x_{1:T}|x_0)logq(x_{1:T}|x_0)}dx_{1:T}\\\\\n",
        "&= E_q[logp_\\theta(x_0|x_{1:T})] -D_{KL}(q(x_{1:T}|x_0)||p(x_{1:T})) + D_{KL}(q(x_{1:T}|x_0)||p(x_{1:T}|x_0))\\\\\n",
        "&≥ E_q[logp_\\theta(x_0|x_{1:T})] -D_{KL}(q(x_{1:T}|x_0)||p(x_{1:T}))  (\\because D_{KL} > 0)\\\\\n",
        "E_q[-logp_\\theta(x_0)] &≤  E_q[-logp_\\theta(x_0|x_{1:T})] + D_{KL}(q(x_{1:T}|x_0)||p(x_{1:T}))\\\\\n",
        "&=  E_q[-logp_\\theta(x_0|x_{1:T})] +  E_q[logq(x_{1:T}|x_0)] -  E_q[logp_\\theta(x_{1:T})]\\\\\n",
        "&= -E_q[{log{{p_\\theta(x_0|x_{1:T})p(x_{1:T})}\\over{q(x_{1:T}|x_0)}}}]\\\\\n",
        "\\therefore -log p_\\theta(x_0) &≤ -E_q[{log{{p_\\theta(x_{0:T})}\\over{q(x_{1:T|x_0})}}}]\\\\ (&\\because logp_\\theta(x_0|x_{1:T})p(x_{1:T}) = logp_\\theta(x_{0:T}))\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "</br>\n",
        "\n",
        "### ***2) Change to tractable form***\n",
        "\n",
        "</br>\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "At, -log p_\\theta(x_0) &≤ -E_q[{log{{p_\\theta(x_{0:T})}\\over{q(x_{1:T|x_0})}}}]\\\\\n",
        "E_q[-log{{p_\\theta(x_{0:T})}\\over{q(x_{1:T}|x_0)}}] &= E_q[-log{{p(x_T)\\Pi_{t=1}^{T}{p(x_{t-1}|x_t)\\over{q(x_t|x_{x_t-1})}}}}]\\\\\n",
        "&= E_q[-logp_\\theta(x_t)-\\sum_{t=1}^{T}{log{{p_\\theta(x_{t-1}|x_0)}\\over{q(x_t|x_{t-1})}}}]\\\\\n",
        "&= E_q[-logp_\\theta(x_t)-\\sum_{t=2}^{T}{log{{{p_\\theta(x_{t-1}|x_0)}\\over{q(x_t|x_{t-1})}}}} - log{{p_\\theta(x_0|x_1)}\\over{p_\\theta(x_1|x_0)}}]\\\\\n",
        "&= E_q[-logp_\\theta(x_t)-\\sum_{t=2}^{T}{log{{{p_\\theta(x_{t-1}|x_0)q(x_{t-1}|x_0)}}\\over{q(x_{t-1}|x_t, x_0)q(x_t|x_0)}}} - log{{p_\\theta(x_0|x_1)}\\over{q(x_1|x_0)}}]\\\\\n",
        "&= E_q[-logp_\\theta(x_t)-\\sum_{t=2}^{T}{log{{{p_\\theta(x_{t-1}|x_0)}}\\over{q(x_{t-1}|x_t, x_0)}}} - log{{q(x_1|x_0)}\\over{x_T|x_0}} - log{{p_\\theta(x_0|x_1)}\\over{q(x_1|x_0)}}]\\\\\n",
        "&= E_q[-log{{p_\\theta(x_t)}\\over{q(x_T|x_0)}}-\\sum_{t=2}^{T}{log{{{p_\\theta(x_{t-1}|x_0)}}\\over{q(x_{t-1}|x_t, x_0)}}} - log{{p_\\theta(x_0|x_1)}}]\\\\\n",
        "&= E_q[D_{KL}(q(x_T|x_0)||p_\\theta(x_T)) + \\sum_{t=2}^{T}{D_{KL}(q(x_{t-1}|x_t, x_0)||p(x_{t-1}|x_t)} - logp_\\theta(x_0|x_1)]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "</br>\n",
        "\n",
        "As $\\beta_t$ is not scheduled, just fixed to a constant value, $E_q[D_{KL}(q(x_T|x_0)||p_\\theta(x_T))$ is also constant\n",
        "\n",
        "</br>\n",
        "\n",
        "### ***3) Simplify training objective!***\n",
        "\n",
        "We will use $D_{KL}(q(x_{t-1}|x_t, x_0)||p_\\theta(x_{t-1}|x_t)))$ for simplifing loss function.  \n",
        "In the paper, the author trained model the part just mentioned by setting it as a training object.  \n",
        "\n",
        "</br>\n",
        "\n",
        "So, our challenge has been narrowed down to training the two distributions($q(x_{t-1}|x_t, x_0), p_\\theta(x_{t-1}|x_t)$) as equally as possible.\n",
        "\n",
        "</br>\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "q(x_{t-1}|x_t, x_0) &= N(x_{t-1}; \\hat{\\mu}(x_t, x_0), \\hat{\\beta}I)\\\\\n",
        "p_\\theta(x_{t-1}|x_t) &= N(x_{t-1}; \\mu_\\theta(x_t, t), {\\sigma_t}^2 I)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "</br>\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "D_{KL}(q(x_{t-1}|x_t, x_0)||p_\\theta(x_{t-1}|x_t))) &= E_{x_0, \\epsilon}[{{{\\beta_t}^2}\\over{2{\\sigma_t}^2 \\alpha_t (1- {\\hat{\\alpha_t}})}}\\left \\|  \\epsilon-\\epsilon_\\theta(\\sqrt{\\hat{\\alpha_t}}x_0 + \\sqrt{1-\\hat{\\alpha_t}}\\epsilon, t)\\right \\|^2]\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "${{{\\beta_t}^2}\\over{2{\\sigma_t}^2 \\alpha_t (1- {\\hat{\\alpha_t}})}}$ is considered as a constant in the paper\n",
        "\n",
        "<center>Final Loss : $E_{x_0, \\epsilon}[\\left \\|  \\epsilon-\\epsilon_\\theta(\\sqrt{\\hat{\\alpha_t}}x_0 + \\sqrt{1-\\hat{\\alpha_t}}\\epsilon, t)\\right \\|^2]$</center>"
      ],
      "metadata": {
        "id": "47DbK7Rqohsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "75_gEfY0n3U8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P0eh8TT0EnDo"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from inspect import isfunction\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as nn\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow import einsum\n",
        "from einops import rearrange\n",
        "from einops.layers.tensorflow import Rearrange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diffusion Model"
      ],
      "metadata": {
        "id": "ITR60v90C6ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gather(c, t):\n",
        "  c = tf.gather(c, t)\n",
        "  return tf.reshape(c, [-1, 1, 1, 1])\n",
        "\n",
        "class DiffusionModel:\n",
        "  def __init__(self, model, n_steps):\n",
        "    super().__init__()\n",
        "    self.eps_model = model\n",
        "    self.beta = tf.linspace(1e-4, 2e-2, n_steps)\n",
        "    self.alpha = 1. - self.beta\n",
        "    self.alpha_hat = tf.math.cumprod(self.alpha, axis=0)\n",
        "    self.n_steps = n_steps\n",
        "    self.sigma = self.beta\n",
        "\n",
        "  def forward_process(self, x_0, t):\n",
        "    mean = gather(self.alpha_hat, t) ** 0.5 *x_0\n",
        "    variance = 1 - gather(self.alpha_hat, t)\n",
        "    return mean, variance\n",
        "\n",
        "  def forward_sample(self, x_0, t, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=x_0.shape)\n",
        "    mean, variance = self.forward_process(x_0, t)\n",
        "    return mean + (variance**0.5)*eps\n",
        "\n",
        "  def reverse_sample(self, x_t, t):\n",
        "    eps_theta = self.model(x_t, t)\n",
        "    alpha_hat = gather(self.alpha_hat, t)\n",
        "    alpha = gather(self.alpha, t)\n",
        "    eps_coef = (1-alpha) / (1-alpha_hat)**0.5\n",
        "\n",
        "    mean = 1 / (alpha**0.5)*(x_t - eps_coef*eps_theta)\n",
        "    variance = gather(self.sigma, t)\n",
        "    eps = tf.random.normal(shape=x_t.shape)\n",
        "    return mean+(variance**0.5)*eps\n",
        "\n",
        "  def loss(self, x_0, noise):\n",
        "    batch_size = x_0.shape[0]\n",
        "    t = tf.random.uniform(shape=[batch_size], minval=0, maxval=self.n_steps, dtype=tf.dtypes.int32)\n",
        "    if noise is None:\n",
        "      noise = tf.random.normal(shape=x_0.shape)\n",
        "    x_t = self.forward_sample(x_0, t, eps=noise)\n",
        "    eps_theta = self.model(x_t, t)\n",
        "    return tf.losses.mse(noise, eps_theta)"
      ],
      "metadata": {
        "id": "KiFaN44XExIC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## U-Net Model for $\\epsilon_\\theta$\n",
        "\n",
        "<img src=\"https://nn.labml.ai/diffusion/ddpm/unet.png\" title=\"U-Net\" width=500>\n",
        "\n",
        "This implementation contains a bunch of modifications to original U-Net  \n",
        "(residual blocks, multi-head attention) and also adds time-step embeddings t."
      ],
      "metadata": {
        "id": "VNRn-IniC-Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "jWM7H-McqLca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helpers functions\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "def cycle(dl):\n",
        "    while True:\n",
        "        for data in dl:\n",
        "            yield data\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "def normalize_to_neg_one_to_one(img):\n",
        "    return img * 2 - 1\n",
        "\n",
        "def unnormalize_to_zero_to_one(t):\n",
        "    return (t + 1) * 0.5\n",
        "\n",
        "# small helper modules\n",
        "class Identity(Layer):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        return tf.identity(x)\n",
        "\n",
        "class EMA(Layer):\n",
        "    def __init__(self, beta=0.995):\n",
        "        super(EMA, self).__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    @tf.function\n",
        "    def update_model_average(self, old_model, new_model):\n",
        "        for old_weight, new_weight in zip(old_model.weights, new_model.weights):\n",
        "            assert old_weight.shape == new_weight.shape\n",
        "\n",
        "            old_weight.assign(self.update_average(old_weight, new_weight))\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new\n",
        "\n",
        "# Residual Func\n",
        "class Residual(Layer):\n",
        "    def __init__(self, fn):\n",
        "        super(Residual, self).__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        return self.fn(x, training=training) + x\n",
        "\n",
        "# U-Net TimeEmbedding\n",
        "class SinusoidalPosEmb(Layer):\n",
        "    def __init__(self, dim, max_positions=10000):\n",
        "        super(SinusoidalPosEmb, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.max_positions = max_positions\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(self.max_positions) / (half_dim - 1)\n",
        "        emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "\n",
        "        emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)\n",
        "\n",
        "        return emb\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.Conv2DTranspose(filters=dim, kernel_size=4, strides=2, padding='SAME')\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2D(filters=dim, kernel_size=4, strides=2, padding='SAME')\n",
        "\n",
        "class LayerNorm(Layer):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "        self.g = tf.Variable(tf.ones([1, 1, 1, dim]))\n",
        "        self.b = tf.Variable(tf.zeros([1, 1, 1, dim]))\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        var = tf.math.reduce_variance(x, axis=-1, keepdims=True)\n",
        "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "\n",
        "        x = (x - mean) / tf.sqrt((var + self.eps)) * self.g + self.b\n",
        "        return x\n",
        "\n",
        "class PreNorm(Layer):\n",
        "    def __init__(self, dim, fn):\n",
        "        super(PreNorm, self).__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = LayerNorm(dim)\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)\n",
        "\n",
        "class SiLU(Layer):\n",
        "    def __init__(self):\n",
        "        super(SiLU, self).__init__()\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        return x * tf.nn.sigmoid(x)\n",
        "\n",
        "def gelu(x, approximate=False):\n",
        "    if approximate:\n",
        "        coeff = tf.cast(0.044715, x.dtype)\n",
        "        return 0.5 * x * (1.0 + tf.tanh(0.7978845608028654 * (x + coeff * tf.pow(x, 3))))\n",
        "    else:\n",
        "        return 0.5 * x * (1.0 + tf.math.erf(x / tf.cast(1.4142135623730951, x.dtype)))\n",
        "\n",
        "\n",
        "class GELU(Layer):\n",
        "    def __init__(self, approximate=False):\n",
        "        super(GELU, self).__init__()\n",
        "        self.approximate = approximate\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        return gelu(x, self.approximate)\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    scale = 1000 / timesteps\n",
        "    beta_start = scale * 0.0001\n",
        "    beta_end = scale * 0.02\n",
        "\n",
        "    return tf.cast(tf.linspace(beta_start, beta_end, timesteps), tf.float32)\n",
        "\n",
        "def cosine_beta_schedule(timesteps, s = 0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule\n",
        "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = tf.cast(tf.linspace(0, timesteps, steps), tf.float32)\n",
        "\n",
        "    alphas_cumprod = tf.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "\n",
        "    return tf.clip_by_value(betas, 0, 0.999)\n",
        "\n",
        "def extract(x, t):\n",
        "    return tf.gather(x, t)[:, None, None, None]"
      ],
      "metadata": {
        "id": "jb6lDXS-6UUU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Block Modules"
      ],
      "metadata": {
        "id": "bkpli7SvqBGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building block modules\n",
        "class Block(Layer):\n",
        "    def __init__(self, dim, groups=8):\n",
        "        super(Block, self).__init__()\n",
        "        self.proj = nn.Conv2D(dim, kernel_size=3, strides=1, padding='SAME')\n",
        "        self.norm = tfa.layers.GroupNormalization(groups, epsilon=1e-05)\n",
        "        self.act = SiLU()\n",
        "\n",
        "    def call(self, x, scale_shift=None, training=True):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x, training=training)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(Layer):\n",
        "    def __init__(self, dim, dim_out, time_emb_dim=None, groups=8):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "\n",
        "        self.mlp = Sequential([\n",
        "            SiLU(),\n",
        "            nn.Dense(units=dim_out * 2)\n",
        "        ]) if exists(time_emb_dim) else None\n",
        "\n",
        "        self.block1 = Block(dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2D(filters=dim_out, kernel_size=1, strides=1) if dim != dim_out else Identity()\n",
        "\n",
        "    def call(self, x, time_emb=None, training=True):\n",
        "        scale_shift = None\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            time_emb = rearrange(time_emb, 'b c -> b 1 1 c')\n",
        "            scale_shift = tf.split(time_emb, num_or_size_splits=2, axis=-1)\n",
        "\n",
        "        h = self.block1(x, scale_shift=scale_shift, training=training)\n",
        "        h = self.block2(h, training=training)\n",
        "\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class LinearAttention(Layer):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super(LinearAttention, self).__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        self.hidden_dim = dim_head * heads\n",
        "\n",
        "        self.attend = nn.Softmax()\n",
        "        self.to_qkv = nn.Conv2D(filters=self.hidden_dim * 3, kernel_size=1, strides=1, use_bias=False)\n",
        "\n",
        "        self.to_out = Sequential([\n",
        "            nn.Conv2D(filters=dim, kernel_size=1, strides=1),\n",
        "            LayerNorm(dim)\n",
        "        ])\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        b, h, w, c = x.shape\n",
        "        qkv = self.to_qkv(x)\n",
        "        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b x y (h c) -> b h c (x y)', h=self.heads), qkv)\n",
        "\n",
        "        q = tf.nn.softmax(q, axis=-2)\n",
        "        k = tf.nn.softmax(k, axis=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        context = einsum('b h d n, b h e n -> b h d e', k, v)\n",
        "\n",
        "        out = einsum('b h d e, b h d n -> b h e n', context, q)\n",
        "        out = rearrange(out, 'b h c (x y) -> b x y (h c)', h=self.heads, x=h, y=w)\n",
        "        out = self.to_out(out, training=training)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super(Attention, self).__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        self.hidden_dim = dim_head * heads\n",
        "\n",
        "        self.to_qkv = nn.Conv2D(filters=self.hidden_dim * 3, kernel_size=1, strides=1, use_bias=False)\n",
        "        self.to_out = nn.Conv2D(filters=dim, kernel_size=1, strides=1)\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        b, h, w, c = x.shape\n",
        "        qkv = self.to_qkv(x)\n",
        "        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b x y (h c) -> b h c (x y)', h=self.heads), qkv)\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n",
        "        sim_max = tf.stop_gradient(tf.expand_dims(tf.argmax(sim, axis=-1), axis=-1))\n",
        "        sim_max = tf.cast(sim_max, tf.float32)\n",
        "        sim = sim - sim_max\n",
        "        attn = tf.nn.softmax(sim, axis=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h d j -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h (x y) d -> b x y (h d)', x = h, y = w)\n",
        "        out = self.to_out(out, training=training)\n",
        "\n",
        "        return out\n",
        "\n",
        "class MLP(Layer):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = Sequential([\n",
        "            Rearrange('... -> ... 1'), # expand_dims(axis=-1)\n",
        "            nn.Dense(units=hidden_dim),\n",
        "            GELU(),\n",
        "            LayerNorm(hidden_dim),\n",
        "            nn.Dense(units=hidden_dim),\n",
        "            GELU(),\n",
        "            LayerNorm(hidden_dim),\n",
        "            nn.Dense(units=hidden_dim),\n",
        "        ])\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        return self.net(x, training=training)"
      ],
      "metadata": {
        "id": "GXQwR-2N6N-Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### U-Net Model"
      ],
      "metadata": {
        "id": "w87Ej93gp50W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Unet(Model):\n",
        "    def __init__(self,\n",
        "                 dim=64,\n",
        "                 init_dim=None,\n",
        "                 out_dim=None,\n",
        "                 dim_mults=(1, 2, 4, 8),\n",
        "                 channels=3,\n",
        "                 resnet_block_groups=8,\n",
        "                 learned_variance=False,\n",
        "                 sinusoidal_cond_mlp=True\n",
        "                 ):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        # determine dimensions\n",
        "        self.channels = channels\n",
        "\n",
        "        init_dim = default(init_dim, dim // 3 * 2)\n",
        "        self.init_conv = nn.Conv2D(filters=init_dim, kernel_size=7, strides=1, padding='SAME')\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n",
        "\n",
        "        # time embeddings\n",
        "        time_dim = dim * 4\n",
        "        self.sinusoidal_cond_mlp = sinusoidal_cond_mlp\n",
        "\n",
        "        if sinusoidal_cond_mlp:\n",
        "            self.time_mlp = Sequential([\n",
        "                SinusoidalPosEmb(dim),\n",
        "                nn.Dense(units=time_dim),\n",
        "                GELU(),\n",
        "                nn.Dense(units=time_dim)\n",
        "            ])\n",
        "        else:\n",
        "            self.time_mlp = MLP(time_dim)\n",
        "\n",
        "        # layers\n",
        "        self.downs = []\n",
        "        self.ups = []\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append([\n",
        "                block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                Downsample(dim_out) if not is_last else Identity()\n",
        "            ])\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.ups.append([\n",
        "                block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
        "                block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                Upsample(dim_in) if not is_last else Identity()\n",
        "            ])\n",
        "\n",
        "        default_out_dim = channels * (1 if not learned_variance else 2)\n",
        "        self.out_dim = default(out_dim, default_out_dim)\n",
        "\n",
        "        self.final_conv = Sequential([\n",
        "            block_klass(dim * 2, dim),\n",
        "            nn.Conv2D(filters=self.out_dim, kernel_size=1, strides=1)\n",
        "        ])\n",
        "\n",
        "    def call(self, x, time=None, training=True, **kwargs):\n",
        "        x = self.init_conv(x)\n",
        "        t = self.time_mlp(time)\n",
        "\n",
        "        h = []\n",
        "\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = tf.concat([x, h.pop()], axis=-1)\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = tf.concat([x, h.pop()], axis=-1)\n",
        "        x = self.final_conv(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mAQs-s8i6aZa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Test!"
      ],
      "metadata": {
        "id": "NhfGkGO3qT32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unet = Unet()\n",
        "diffusion = DiffusionModel(unet, 1000)"
      ],
      "metadata": {
        "id": "ZH8P3UBkRPir"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tfds.load('cifar10')"
      ],
      "metadata": {
        "id": "XbJ1-cTaSOUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = next(iter(dataset['train']))['image'].numpy()"
      ],
      "metadata": {
        "id": "CO2E-k4VVA3C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "ax1 = fig.add_subplot(1, 3, 1)\n",
        "ax1.imshow(sample/255.)\n",
        "ax1.set_title('Original Image')\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "ax2 = fig.add_subplot(1, 3, 2)\n",
        "ax2.imshow(diffusion.forward_sample(sample/255., 10).numpy().reshape([32,32,3]))\n",
        "ax2.set_title('step 10')\n",
        "ax2.axis(\"off\")\n",
        "\n",
        "ax3 = fig.add_subplot(1, 3, 3)\n",
        "ax3.imshow(diffusion.forward_sample(sample/255., 50).numpy().reshape([32,32,3]))\n",
        "ax3.set_title('step 50')\n",
        "ax3.axis('off')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "ldXZkzoBVYA9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "aaa19f15-e912-48cf-fc52-51631515e8e9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OdZ1iP6htJ44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(diffusion.forward_sample(sample/255., 100).numpy().reshape(32, 32, 3))"
      ],
      "metadata": {
        "id": "0NmDSsXeVwYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JeW8c9SpWFkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}